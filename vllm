export VLLM_API_KEY=mysecrettoken123
export CUDA_VISIBLE_DEVICES=2,3


# python -m vllm.entrypoints.openai.api_server \
#   --model /mnt/workspace/pretrain_model/qwen3/Qwen/Qwen3-14B \
#   --served-model-name Qwen3-14B \
#   --enable-auto-tool-choice \
#   --tool-call-parser hermes \
#   --tensor-parallel-size 1 \
#   --max-num-seqs 32 \
#   --gpu-memory-utilization 0.9 \
#   --max-model-len 40960 \
#   --host 0.0.0.0 \
#   --port 8000

# python -m vllm.entrypoints.openai.api_server \
#   --model /mnt/workspace/pretrain_model/mistralai/Devstral-Small-2507 \
#   --served-model-name Devstral-Small-2507 \
#   --tokenizer_mode mistral \
#   --config_format mistral \
#   --load_format mistral \
#   --tool-call-parser mistral \
#   --enable-auto-tool-choice \
#   --tensor-parallel-size 1 \
#   --max-num-seqs 32 \
#   --gpu-memory-utilization 0.85 \
#   --max-model-len 40960 \
#   --host 0.0.0.0 \
#   --port 8001

python -m vllm.entrypoints.openai.api_server \
  --model /data/pretrained_models/Qwen/Qwen3-32B \
  --served-model-name Qwen3-32B \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --tensor-parallel-size 2 \
  --max-num-seqs 32 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 40960 \
  --host 0.0.0.0 \
  --port 8002

# python -m vllm.entrypoints.openai.api_server \
#   --model /mnt/workspace/pretrain_model/THUDM/GLM-4-9B-0414 \
#   --served-model-name GLM-4-9B-0414 \
#   --enable-auto-tool-choice \
#   --tool-call-parser hermes \
#   --tensor-parallel-size 1 \
#   --max-num-seqs 32 \
#   --gpu-memory-utilization 0.9 \
#   --max-model-len 32768 \
#   --host 0.0.0.0 \
#   --port 8003

# python -m vllm.entrypoints.openai.api_server \
#   --model /mnt/workspace/pretrain_model/osllmai-community/Llama-3.3-70B-Instruct \
#   --served-model-name Llama-3.3-70B-Instruct \
#   --enable-auto-tool-choice \
#   --tool-call-parser llama3_json \
#   --tensor-parallel-size 4 \
#   --max-num-seqs 32 \
#   --gpu-memory-utilization 0.9 \
#   --max-model-len 40960 \
#   --host 0.0.0.0 \
#   --port 8004

# python -m vllm.entrypoints.openai.api_server \
#   --model /mnt/workspace/pretrain_model/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B \
#   --served-model-name DeepSeek-R1-0528-Qwen3-8B \
#   --enable-auto-tool-choice \
#   --tool-call-parser hermes \
#   --tensor-parallel-size 1 \
#   --max-num-seqs 32 \
#   --gpu-memory-utilization 0.9 \
#   --max-model-len 40960 \
#   --host 0.0.0.0 \
#   --port 8005

# python -m vllm.entrypoints.openai.api_server \
#   --model /mnt/workspace/pretrain_model/qwen3/Qwen/Qwen3-8B \
#   --served-model-name Qwen3-8B \
#   --enable-auto-tool-choice \
#   --tool-call-parser hermes \
#   --tensor-parallel-size 1 \
#   --max-num-seqs 32 \
#   --gpu-memory-utilization 0.9 \
#   --max-model-len 40960 \
#   --host 0.0.0.0 \
#   --port 8006

# python -m vllm.entrypoints.openai.api_server \
#   --model /mnt/workspace/pretrain_model/meta-llama/Meta-Llama-3.1-8B-Instruct \
#   --served-model-name Meta-Llama-3.1-8B-Instruct \
#   --enable-auto-tool-choice \
#   --tool-call-parser llama3_json \
#   --tensor-parallel-size 2 \
#   --max-num-seqs 32 \
#   --gpu-memory-utilization 0.9 \
#   --max-model-len 40960 \
#   --host 0.0.0.0 \
#   --port 8007

# python -m vllm.entrypoints.openai.api_server \
#   --model /mnt/workspace/pretrain_model/THUDM/GLM-4-32B-0414 \
#   --served-model-name GLM-4-32B-0414 \
#   --enable-auto-tool-choice \
#   --tool-call-parser hermes \
#   --tensor-parallel-size 2 \
#   --max-num-seqs 32 \
#   --gpu-memory-utilization 0.9 \
#   --max-model-len 32768 \
#   --host 0.0.0.0 \
#   --port 8008