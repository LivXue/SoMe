<img src="https://github.com/LivXue/SoMe/blob/main/pics/logo.png" alt="SoMe Benchmark Logo" width="33%">
# ğŸ¤– SoMe: A Realistic Benchmark for LLM-based Social Media Agents

## ğŸ“‹ Overview

SoMe is a comprehensive benchmark designed to evaluate the capabilities of Large Language Model (LLM)-based agents in realistic social media scenarios. This benchmark provides a standardized framework for testing and comparing social media agents across multiple dimensions of performance.

![SoMe Benchmark Overview](https://github.com/LivXue/SoMe/blob/main/pics/framework.png)

## ğŸ“° News

- **[2026.02]** ğŸ‰ Our paper is accepted by AAAI 2026!

## âœ¨ Features

SoMe benchmark includes evaluation of social media agents across the following key tasks:

- **ğŸš¨ Realtime Event Detection** - Assessing how well agents can identify and track emerging events in real-time
- **ğŸ“Š Streaming Event Summary** - Evaluating the agent's capacity to summarize ongoing events from streaming data
- **ğŸš« Misinformation Detection** - Testing the agent's capability to identify and flag potentially false or misleading information
- **ğŸ¯ User Behavior Prediction** - Evaluating how well agents can predict user interactions with social media content
- **ğŸ˜Š User Emotion Analysis** - Assessing the agent's ability to analyze user emotions towards social media content
- **ğŸ’¬ User Comment Simulation** - Testing how realistically agents can simulate user comments
- **ğŸ“± Media Content Recommendation** - Evaluating an agent's ability to recommend relevant media content to users based on their interests and preferences
- **â“ Social Media Question Answering** - Measuring the agent's ability to accurately answer questions about social media content

## ğŸ“ˆ Dataset Statistics

The SoMe benchmark includes comprehensive datasets for each task, with the following statistics:

| Task | #Query | #Data | Data Type |
|------|-------------|-----------|----------|
| ğŸš¨ Real-time Event Detection | 568 | 476,611 | Posts |
| ğŸ“Š Streaming Event Summary | 154 | 7,898,959 | Posts |
| ğŸš« Misinformation Detection | 1,451 | 27,137 | Posts & Knowledge |
| ğŸ¯ User Behavior Prediction | 3,000 | 840,200 | Posts & Users|
| ğŸ˜Š User Emotion Analysis | 2,696 | 840,200 | Posts & Users |
| ğŸ’¬ User Comment Simulation | 4,000 | 840,200 | Posts & Users |
| ğŸ“± Media Content Recommendation | 4,000 | 840,200 | Posts & Users |
| â“ Social Media Question Answering | 2,000 | 8,651,759 | Posts & Users |
| **Total** | **17,869** | **9,242,907** | **All** |

## ğŸ“ Project Structure

```
Social-Media-Agent/
â”œâ”€â”€ ğŸ¤– agent.py                    # Main social media agent implementation
â”œâ”€â”€ ğŸ”§ qwen_agent/                 # Qwen-Agent library
â”œâ”€â”€ ğŸ“‹ tasks/                      # Task-specific modules
â”‚   â”œâ”€â”€ ğŸ“± media_content_recommend/
â”‚   â”œâ”€â”€ ğŸš« misinformation_detection/
â”‚   â”œâ”€â”€ ğŸš¨ realtime_event_detection/
â”‚   â”œâ”€â”€ â“ social_media_question_answering/
â”‚   â”œâ”€â”€ ğŸ“Š streaming_event_summary/
â”‚   â”œâ”€â”€ ğŸ’¬ user_comment_simulation/
â”‚   â”œâ”€â”€ ğŸ˜Š user_emotion_analysis/
â”‚   â””â”€â”€ ğŸ¯ user_behavior_prediction/
â”œâ”€â”€ ğŸ› ï¸ tools/                      # Tools for social media analysis
â”œâ”€â”€ ğŸ§ª test_*.py                   # Test scripts for each task
â”œâ”€â”€ ğŸ“Š eval_scripts/               # Evaluation scripts for scoring
â”œâ”€â”€ ğŸ“‚ results/                    # Directory for storing results
â”œâ”€â”€ ğŸ“Š datasets/                   # Dataset directory
â””â”€â”€ ğŸ’¾ database/                   # Database directory
```

## ğŸš€ Installation

1. ğŸ“¥ Clone the repository:
```bash
git clone https://github.com/your-username/Social-Media-Agent.git
cd Social-Media-Agent
```

2. ğŸ“¦ Install the required dependencies:
```bash
pip install -r requirements.txt
```

3. ğŸ”§ Install the qwen-agent package:
```bash
pip install -e ./qwen_agent
```

4. ğŸ“¥ Download the test data and unzip it into `database`
   - Google Drive: [https://drive.google.com/file/d/1sD2EaZStK5nODQWlJTHZ8WfFb5QHgwMN/view?usp=drive_link](https://drive.google.com/file/d/1sD2EaZStK5nODQWlJTHZ8WfFb5QHgwMN/view?usp=drive_link)  
   - Baidu Disk: [https://pan.baidu.com/s/1DugTyLR5AaQHeOdXG6wqQQ?pwd=SoMe](https://pan.baidu.com/s/1DugTyLR5AaQHeOdXG6wqQQ?pwd=SoMe) SoMe



## ğŸ’» Usage

### ğŸƒâ€â™‚ï¸ Running Individual Tasks

Each task can be evaluated using its corresponding test script:

```bash
# ğŸš¨ Realtime Event Detection
python test_realtime_event_detection.py --model MODEL_NAME --base_url MODEL_SERVER_URL --api_key API_KEY

# ğŸ“Š Streaming Event Summary
python test_streaming_event_summary.py --model MODEL_NAME --base_url MODEL_SERVER_URL --api_key API_KEY

# ğŸš« Misinformation Detection
python test_misinformation_detection.py --model MODEL_NAME --base_url MODEL_SERVER_URL --api_key API_KEY

# ğŸ¯ User bahavior Prediction
python test_user_behavior_prediction.py --model MODEL_NAME --base_url MODEL_SERVER_URL --api_key API_KEY

# ğŸ˜Š User Emotion Analysis
python test_user_emotion_analysis.py --model MODEL_NAME --base_url MODEL_SERVER_URL --api_key API_KEY

# ğŸ’¬ User Comment Simulation
python test_user_comment_simulation.py --model MODEL_NAME --base_url MODEL_SERVER_URL --api_key API_KEY

# ğŸ“± Media Content Recommendation
python test_media_content_recommend.py --model MODEL_NAME --base_url MODEL_SERVER_URL --api_key API_KEY

# â“ Social Media Question Answering
python test_social_media_question_answering.py --model MODEL_NAME --base_url MODEL_SERVER_URL --api_key API_KEY
```

### âš™ï¸ Command Line Arguments

- `--model`: The model name to use (e.g., "deepseek-chat")
- `--base_url`: The base URL for the model server (e.g., "https://api.deepseek.com")
- `--api_key`: The API key for the model server
- `--output_path`: The output path for results (default: "results/[task_name]")

### ğŸ“Š Evaluation

After running the test scripts, you can evaluate the results using the provided evaluation scripts:

```bash
# For each task, use the corresponding evaluation script
python eval_scripts/[TASK]_extraction.py
python eval_scripts/[TASK]_compute_score.py
# Or
python eval_scripts/[TASK]_scoring.py
python eval_scripts/[TASK]_compute_score.py
```
***Note***: The LLM setting of evaluation is in `eval_scripts/settings.json`

## ğŸ§  Model Support

The benchmark supports various LLM models including:

- ğŸ§© Qwen series models
- ğŸ”Œ OpenAI compatible models
- ğŸŒ Any model that provides an OpenAI-compatible API endpoint

## ğŸ“š Citation

If you use this benchmark in your research, please cite:

```
@inproceedings{some2026,
  title={SoMe: A Realistic Benchmark for LLM-based Social Media Agents},
  author={Dizhan Xue, Jing Cui, Shengsheng Qian, Chuanrui Hu, Changsheng Xu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2026}
}
```

## ğŸ¤ Contributing

We welcome contributions to improve the benchmark! Please feel free to submit pull requests or open issues for any bugs or feature requests.

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

This benchmark is built upon the Qwen-Agent framework. We thank the Qwen team for their excellent work on developing this framework.